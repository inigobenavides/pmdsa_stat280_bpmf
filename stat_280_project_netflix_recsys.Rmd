---
title: "STAT 280 Project Netflix Bayesian Probabilistic Matrix Factorization"
author: "Inigo Benavides"
date: "11/3/2019"
output: html_document
---

```{r setup, include=FALSE}
library("tidyverse")
library("DBI")
library("RSQLite")
library("feather")
library("mvtnorm")
library("glue")
source("code/visualisations.R")
source("code/utils.R")
source("code/bpmf_utils.R")
source("code/bpmf_gibbs_sampler.R")
```

# Bayesian Probabilistic Matrix Factorization

In this project, we implement a Gibbs sampler to generate recommendations for Netflix user ratings on movies, based on the Movielens data set.

```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# ETL csv to feather
files_to_convert <- c("movie", "link", "tag", "genome_scores", "genome_tags", "rating")
path <- "/home/rstudio/data/movielens"

for (file in files_to_convert) {
  # Read CSV
  csv_path <- path %>% paste(file, sep="/") %>% paste(".csv", sep="")
  csv_file <- read_csv(csv_path)
  
  # Write feather
  feather_path <- path %>% paste(file, sep="/") %>% paste(".feather", sep="")
  feather::write_feather(csv_file, feather_path)
}
```

## Exploratory Data Analysis

```{r cache=TRUE}
# Load ratings
ratings <- read_feather("/home/rstudio/data/movielens/rating.feather")
ratings %>% head
```

```{r cache=TRUE}
# Summarise ratings
ratings %>% summarise(nrows=n(),
                      ndistinct_users=n_distinct(userId),
                      ndistinct_movies=n_distinct(movieId),
                      min_date=min(timestamp),
                      max_date=max(timestamp),
                      min_rating=min(rating),
                      mean_rating=mean(rating),
                      max_rating=max(rating))
```



```{r}
# Load movies
movies <- read_feather("/home/rstudio/data/movielens/movie.feather")
movies %>% head
```


```{r}
# Top 100 Movies by mean ratings with at least 1000 
ratings %>%
  group_by(movieId) %>%
  summarise(mean_rating=mean(rating), count=n()) %>% 
  filter(count > 1000) %>% 
  arrange(desc(mean_rating)) %>% 
  head(100) %>% 
  left_join(movies, by=c("movieId")) %>% 
  select(title, mean_rating, count)
```


```{r}
ratings %>%
  group_by(movieId) %>%
  summarise(mean_rating=mean(rating), count=n()) %>% 
  sample_n(2000) %>% 
  ggplot(aes(x=log10(count), y=mean_rating)) + geom_point() + geom_smooth() + theme_minimal()
```

## Model

In this section, we specify the model for Bayesian Probalistic Matrix Factorization (Salakhutdinov and Minh, 2008) as follows:

We assume that each movie rating $r_{ij}$ made by user $i \in \{1,\cdots, N\}$ for movie $j \in \{1,\cdots, M\}$ is normally distributed with mean as a linear combination of a $k$-rank latent user vector $U_i$ and latent movie vector $V_j$, whose elements are Gaussian distributed; as well as with precision $\alpha$.

$$
p(R|U, V, \alpha) = \prod_{i=1}^{N} \prod_{j=1}^M \left[\mathcal{N}(R_{ij} \mid U_i^TV_j, \alpha^{-1})\right]^{I_{ij}}
$$
where $I_{ij}$ is the indicator variable equal to 1 if user $i$ rated movie $j$, and 0 otherwise. We place Gaussian priors on the elements of each latent feature vector for users and movies with hyperparameters $\mu_{U}, \Lambda_{U}$ and $\mu_{V}, \Lambda_{V}$:
$$
p(U \mid \mu_{U}, \Lambda_{U}) = \prod_{i=1}^{N} \mathcal{N}(U_{i} \mid \mu_{U}, \Lambda_{U}^{-1})
$$

and 

$$
p(V \mid \mu_{V}, \Lambda_{V}) = \prod_{j=1}^{M} \mathcal{N}(V_{j} \mid \mu_{V}, \Lambda_{V}^{-1})
$$

Here $U_i$ defines is a vector of length $k$ that represents an embedding of a user $i$, while $V_j$ similarly defines a vector of length $k$ that represents an embedding of a movie $j$. 

We further place Gaussian-Wishart priors on $\mu_{U}, \Lambda_{U}$ and $\mu_{V}, \Lambda_{V}$:

$$
p(\mu_{U}, \Lambda_{U} \mid \mu_{0}, \Lambda_{0}) = p(\mu_{U} \mid \Lambda_{U}) p(\Lambda_{U}) = \mathcal{N}(\mu_{U} \mid \mu_{0}, (\beta_0\Lambda_{U})^{-1}) \mathcal{W}(\Lambda_{U}|W_0, \nu_0)
$$
$$
p(\mu_{V}, \Lambda_{V} \mid \mu_{0}, \Lambda_{0}) = p(\mu_{U} \mid \Lambda_{V}) p(\Lambda_{V}) = \mathcal{N}(\mu_{V} \mid \mu_{0}, (\beta_0\Lambda_{V})^{-1}) \mathcal{W}(\Lambda_{V}|W_0, \nu_0)
$$

where $\mathcal{W}$ is the Wishart distribution with $\nu_0$ degrees of freedom and a $k$ x  $k$ scale matrix $W_0$:

$$
\mathcal{W \mid W_0, \nu_0} \propto |\Lambda|^{(\nu_0-D-1)/2} \exp{(\frac{1}{2}\text{Tr}(W_0^{-1}\Lambda))}
$$

This distribution is a multivariate generalization of the gamma distribution.

## Methodology

By specifying the model using conjugate priors, we can analytically compute the posterior conditional distributions, which we can then use with Gibbs sampling. Thus, the conditional distribution for $U_i$ is:

$$
p(U_i \mid R, V, \mu_U, \Lambda_U, \alpha) = \mathcal{N}(U_i \mid \mu_i^{*}, [\Lambda_i^{*}]^{-1})
$$
where,
$$
\Lambda_{i}^{*} = \Lambda_U + \alpha \sum_{j=1}^{M}{[V_jV_j^T]^{I_{ij}}}
$$
and
$$
\mu_i^{*} = [\Lambda_{i}^{*}]^{-1} \left(\alpha \sum_{j=1}^{M} {[V_jR_{ij}]^{I_{ij}}} + \Lambda_U \mu_U \right)
$$

Since each user vector $U_i$ is assumed independent, it follows that:

$$
p(U \mid R, V, \mu_U, \Lambda_U) = \prod_{i=1}^{N} {p(U_i \mid R, V, \mu_U, \Lambda_U, \alpha)}
$$

By similar reasoning, we can derive the conditional distribution for $V_j$:

$$
p(V_j \mid R, U, \mu_V, \Lambda_V, \alpha) = \mathcal{N}(V_j \mid \mu_j^{*}, [\Lambda_j^{*}]^{-1})
$$
where,
$$
\Lambda_{j}^{*} = \Lambda_V + \alpha \sum_{i=1}^{N}{[U_iU_i^T]^{I_{ij}}}
$$
and
$$
\mu_j^{*} = [\Lambda_{j}^{*}]^{-1} \left(\alpha \sum_{i=1}^{N} {[U_iR_{ij}]^{I_{ij}}} + \Lambda_V \mu_V \right)
$$

Lastly, we derive the conditional distribution for the hyperparameters $\mu_{U}, \Lambda_{U}$ and $\mu_{V}, \Lambda_{V}$:

$$
p(\mu_{U}, \Lambda_{U} \mid U, \mu_0, \Lambda_0) = \mathcal{N}(\mu_U \mid \mu_0^{*}, (\beta_0^*\Lambda_U)^{-1}) \mathcal{W}(\Lambda_U \mid W_0^*, \nu_0^*)
$$

where,

$$
\begin{aligned}
u_0^* = \frac{\beta_0\mu_0 + N\bar{U}}{\beta_0+N} \\
\beta_0^* = \beta_0 + N \\
\nu_0^* = \nu_0 + N \\
[W_0^*]^{-1} = W_0^{-1} + N\bar{S} + \frac{\beta_0N}{\beta_0 + N}(\mu_0 - \bar{U})(\mu_0 - \bar{U})^T
\end{aligned}
$$

where the sample mean and variances are given by:

$$
\begin{aligned}
\bar{U} = \frac{1}{N} \sum_{i=1}^{N}{U_i} \\
\bar{S} = \frac{1}{N} \sum_{i=1}^{N}{U_iU_i^T}
\end{aligned}
$$

We can similarly solve for the conditional posterior distributions for $\mu_{V}, \Lambda_{V}$, following the above form.

### Gibbs Sampler Algorithm

Since we have specified the conditional distribution of the parameters hyperparameters, we can apply Gibbs sampling as follows:

1. Initialize model parameters ${U^{1}, V^{1}}$

2. For $t=1,\cdots,T$ iterations:

a.) Sample hyperparameters $\mu_U^{(t)}, \Lambda_U^{(t)} \sim p(\mu_U, \Lambda_U \mid U^{(t)}, \mu_0, \Lambda_0)$ and $\mu_V^{(t)}, \Lambda_V^{(t)} \sim p(\mu_V, \Lambda_V \mid V^{(t)}, \mu_0, \Lambda_0)$

b.) For each $i = 1, \cdots, N$, sample user features in parallel $U_i^{(t+1)} \sim p(U_i \mid R, V^{(t)}, \mu_U^{(t)}, \Lambda_U^{(t)})$

c.) For each $j = 1, \cdots, M$, sample movie features in parallel $V_j^{(t+1)} \sim p(V_j \mid R, U^{(t+1)}, \mu_V^{(t)}, \Lambda_V^{(t)})$

## Experiment on Synthetic Data

```{r}
# Generate synthetic ratings matrix
k <- 3
n_users <- 4
n_movies <- 5
U_synthetic <- 1:n_users %>% Map(function(x) {
    sampled_covariance <- rWishart(1, k, diag(k))[,,1]
    rmvnorm(1, mean=rep(0, k), sigma=sampled_covariance)
  }, .) %>% Reduce(function(x, y){
    rbind(x, y)
  }, .)

V_synthetic <- 1:n_movies %>% Map(function(x) {
    sampled_covariance <- rWishart(1, k, diag(k))[,,1]
    rmvnorm(1, mean=rep(0, k), sigma=sampled_covariance)
  }, .) %>% Reduce(function(x, y){
    rbind(x, y)
  }, .)

R_synthetic <- U_synthetic %*% t(V_synthetic)
R_synthetic

R_synthetic %>% 
  matrix_to_tidydf() %>% 
  vis_matrix(row_col = 'row', column_col = 'col',
             value_col = 'value') + 
  labs(y = 'Users', x = 'Movies')
```


```{r}
# Randomly nk sample entries from synthetic ratings matrix
# For every row, sample at least 3 ratings
sparse_observations <- c()
for (i in 1:n_users) {
  index_sample <- sample(1:n_movies, size=2)
  for (j in index_sample) {
    sampled_tuple <- c(i, j, R_synthetic[i, j]) %>% as.matrix() %>% t
    sparse_observations <- rbind(sparse_observations, sampled_tuple)
  }
}
sparse_observations %>% 
  vis_matrix(row_col = "V1", column_col = "V2",
             value_col = "V3") +
  labs(y = 'Users', x = 'Movies')
```


```{r}
# Implement Gibbs sampler
n_replications <- 1000

# Define initial parameters
k_estimate <- 3
alpha <- 10
mu_0 <- rep(0, k_estimate)
beta_0 <- 1
nu_0 <- 1
W_0 <- diag(k_estimate)

# Run
simulation_results <- BPMF_Gibbs_Sampler(
  sparse_observations, 
  k_estimate, 
  n_replications, 
  n_users, 
  n_movies, 
  mu_0, beta_0, nu_0, W_0, alpha
)

```

```{r}
index_i <- 4
index_j <- 3
xs <- 500:n_replications
sampled_rs <- xs %>% Map(function(x) {simulation_results$Rs[[x]][index_i,index_j]}, .) %>% unlist
posterior_rs_mean <- mean(sampled_rs)
data.frame(
  x=xs,
  y=sampled_rs
  ) %>% ggplot(aes(x=y)) + geom_density() + theme_minimal() + geom_vline(xintercept=posterior_rs_mean) + geom_vline(xintercept=R_synthetic[index_i, index_j], color='red') + labs(title=posterior_rs_mean)
```



