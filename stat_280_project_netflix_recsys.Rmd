---
title: "STAT 280 Project Netflix Bayesian Probabilistic Matrix Factorization"
author: "Inigo Benavides"
date: "11/3/2019"
output: html_document
---

```{r setup, include=FALSE}
library("tidyverse")
library("DBI")
library("RSQLite")
library("feather")
library("mvtnorm")
library("glue")
library("purrr")
library("rlang")
library("magrittr")
library("logger")
walk(list.files("code", full.names = TRUE), .f = function(x) {source(x)})
```

# Bayesian Probabilistic Matrix Factorization

In this project, we implement a Gibbs sampler to generate recommendations for Netflix user ratings on movies, based on the Movielens data set.

```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# ETL csv to feather
files_to_convert <- c("movie", "link", "tag", "genome_scores", "genome_tags", "rating")
path <- "/home/rstudio/data/movielens"

for (file in files_to_convert) {
  # Read CSV
  csv_path <- path %>% paste(file, sep="/") %>% paste(".csv", sep="")
  csv_file <- read_csv(csv_path)
  
  # Write feather
  feather_path <- path %>% paste(file, sep="/") %>% paste(".feather", sep="")
  feather::write_feather(csv_file, feather_path)
}
```

## Exploratory Data Analysis

```{r cache=TRUE}
# Load ratings
ratings <- read_feather("/home/rstudio/data/movielens/rating.feather")
ratings %>% head
```

```{r cache=TRUE}
# Summarise ratings
ratings %>% summarise(nrows=n(),
                      ndistinct_users=n_distinct(userId),
                      ndistinct_movies=n_distinct(movieId),
                      min_date=min(timestamp),
                      max_date=max(timestamp),
                      min_rating=min(rating),
                      mean_rating=mean(rating),
                      max_rating=max(rating))
```




```{r}
# Load movies
movies <- read_feather("/home/rstudio/data/movielens/movie.feather")
movies %>% head
```


```{r}
# Top 100 Movies by mean ratings with at least 1000 
ratings %>%
  group_by(movieId) %>%
  summarise(mean_rating=mean(rating), count=n()) %>%
  filter(count >= 1000) %>% 
  arrange(desc(mean_rating)) %>% 
  head(100) %>% 
  left_join(movies, by=c("movieId")) %>% 
  select(title, mean_rating, count)
```

## Model

In this section, we specify the model for Bayesian Probalistic Matrix Factorization (Salakhutdinov and Minh, 2008) as follows:

We assume that each movie rating $r_{ij}$ made by user $i \in \{1,\cdots, N\}$ for movie $j \in \{1,\cdots, M\}$ is normally distributed with mean as a linear combination of a $k$-rank latent user vector $U_i$ and latent movie vector $V_j$, whose elements are Gaussian distributed; as well as with precision $\alpha$.

$$
p(R|U, V, \alpha) = \prod_{i=1}^{N} \prod_{j=1}^M \left[\mathcal{N}(R_{ij} \mid U_i^TV_j, \alpha^{-1})\right]^{I_{ij}}
$$
where $I_{ij}$ is the indicator variable equal to 1 if user $i$ rated movie $j$, and 0 otherwise. We place Gaussian priors on the elements of each latent feature vector for users and movies with hyperparameters $\mu_{U}, \Lambda_{U}$ and $\mu_{V}, \Lambda_{V}$:
$$
p(U \mid \mu_{U}, \Lambda_{U}) = \prod_{i=1}^{N} \mathcal{N}(U_{i} \mid \mu_{U}, \Lambda_{U}^{-1})
$$

and 

$$
p(V \mid \mu_{V}, \Lambda_{V}) = \prod_{j=1}^{M} \mathcal{N}(V_{j} \mid \mu_{V}, \Lambda_{V}^{-1})
$$

Here $U_i$ defines is a vector of length $k$ that represents an embedding of a user $i$, while $V_j$ similarly defines a vector of length $k$ that represents an embedding of a movie $j$. 

We further place Gaussian-Wishart priors on $\mu_{U}, \Lambda_{U}$ and $\mu_{V}, \Lambda_{V}$:

$$
p(\mu_{U}, \Lambda_{U} \mid \mu_{0}, \Lambda_{0}) = p(\mu_{U} \mid \Lambda_{U}) p(\Lambda_{U}) = \mathcal{N}(\mu_{U} \mid \mu_{0}, (\beta_0\Lambda_{U})^{-1}) \mathcal{W}(\Lambda_{U}|W_0, \nu_0)
$$
$$
p(\mu_{V}, \Lambda_{V} \mid \mu_{0}, \Lambda_{0}) = p(\mu_{U} \mid \Lambda_{V}) p(\Lambda_{V}) = \mathcal{N}(\mu_{V} \mid \mu_{0}, (\beta_0\Lambda_{V})^{-1}) \mathcal{W}(\Lambda_{V}|W_0, \nu_0)
$$

where $\mathcal{W}$ is the Wishart distribution with $\nu_0$ degrees of freedom and a $k$ x  $k$ scale matrix $W_0$:

$$
\mathcal{W \mid W_0, \nu_0} \propto |\Lambda|^{(\nu_0-D-1)/2} \exp{(\frac{1}{2}\text{Tr}(W_0^{-1}\Lambda))}
$$

This distribution is a multivariate generalization of the gamma distribution.

## Methodology

By specifying the model using conjugate priors, we can analytically compute the posterior conditional distributions, which we can then use with Gibbs sampling. Thus, the conditional distribution for $U_i$ is:

$$
p(U_i \mid R, V, \mu_U, \Lambda_U, \alpha) = \mathcal{N}(U_i \mid \mu_i^{*}, [\Lambda_i^{*}]^{-1})
$$
where,
$$
\Lambda_{i}^{*} = \Lambda_U + \alpha \sum_{j=1}^{M}{[V_jV_j^T]^{I_{ij}}}
$$
and
$$
\mu_i^{*} = [\Lambda_{i}^{*}]^{-1} \left(\alpha \sum_{j=1}^{M} {[V_jR_{ij}]^{I_{ij}}} + \Lambda_U \mu_U \right)
$$

Since each user vector $U_i$ is assumed independent, it follows that:

$$
p(U \mid R, V, \mu_U, \Lambda_U) = \prod_{i=1}^{N} {p(U_i \mid R, V, \mu_U, \Lambda_U, \alpha)}
$$

By similar reasoning, we can derive the conditional distribution for $V_j$:

$$
p(V_j \mid R, U, \mu_V, \Lambda_V, \alpha) = \mathcal{N}(V_j \mid \mu_j^{*}, [\Lambda_j^{*}]^{-1})
$$
where,
$$
\Lambda_{j}^{*} = \Lambda_V + \alpha \sum_{i=1}^{N}{[U_iU_i^T]^{I_{ij}}}
$$
and
$$
\mu_j^{*} = [\Lambda_{j}^{*}]^{-1} \left(\alpha \sum_{i=1}^{N} {[U_iR_{ij}]^{I_{ij}}} + \Lambda_V \mu_V \right)
$$

Lastly, we derive the conditional distribution for the hyperparameters $\mu_{U}, \Lambda_{U}$ and $\mu_{V}, \Lambda_{V}$:

$$
p(\mu_{U}, \Lambda_{U} \mid U, \mu_0, \Lambda_0) = \mathcal{N}(\mu_U \mid \mu_0^{*}, (\beta_0^*\Lambda_U)^{-1}) \mathcal{W}(\Lambda_U \mid W_0^*, \nu_0^*)
$$

where,

$$
\begin{aligned}
u_0^* = \frac{\beta_0\mu_0 + N\bar{U}}{\beta_0+N} \\
\beta_0^* = \beta_0 + N \\
\nu_0^* = \nu_0 + N \\
[W_0^*]^{-1} = W_0^{-1} + N\bar{S} + \frac{\beta_0N}{\beta_0 + N}(\mu_0 - \bar{U})(\mu_0 - \bar{U})^T
\end{aligned}
$$

where the sample mean and variances are given by:

$$
\begin{aligned}
\bar{U} = \frac{1}{N} \sum_{i=1}^{N}{U_i} \\
\bar{S} = \frac{1}{N} \sum_{i=1}^{N}{U_iU_i^T}
\end{aligned}
$$

We can similarly solve for the conditional posterior distributions for $\mu_{V}, \Lambda_{V}$, following the above form.

### Gibbs Sampler Algorithm

Since we have specified the conditional distribution of the parameters hyperparameters, we can apply Gibbs sampling as follows:

1. Initialize model parameters ${U^{1}, V^{1}}$

2. For $t=1,\cdots,T$ iterations:

a.) Sample hyperparameters $\mu_U^{(t)}, \Lambda_U^{(t)} \sim p(\mu_U, \Lambda_U \mid U^{(t)}, \mu_0, \Lambda_0)$ and $\mu_V^{(t)}, \Lambda_V^{(t)} \sim p(\mu_V, \Lambda_V \mid V^{(t)}, \mu_0, \Lambda_0)$

b.) For each $i = 1, \cdots, N$, sample user features in parallel $U_i^{(t+1)} \sim p(U_i \mid R, V^{(t)}, \mu_U^{(t)}, \Lambda_U^{(t)})$

c.) For each $j = 1, \cdots, M$, sample movie features in parallel $V_j^{(t+1)} \sim p(V_j \mid R, U^{(t+1)}, \mu_V^{(t)}, \Lambda_V^{(t)})$

## Experiment on Synthetic Data

```{r}
# Generate synthetic ratings matrix
k <- 3
n_users <- 4
n_movies <- 10
U_synthetic <- generate_synthetic_matrix(n_users, k)
V_synthetic <- generate_synthetic_matrix(n_movies, k)
R_synthetic <- U_synthetic %*% t(V_synthetic)
R_synthetic_tidy <- R_synthetic %>% 
  matrix_to_tidydf()

R_synthetic_tidy %>% 
  mutate(value = sapply(value, transform_score_to_rating)) %>% 
  vis_matrix(row_col = 'row', column_col = 'col',
             value_col = 'value', label = TRUE) + 
  labs(y = 'Users', x = 'Movies')
```


```{r}
# Randomly nk sample entries from synthetic ratings matrix
# For every row, sample at least 3 ratings
sparse_observations <- R_synthetic_tidy %>% 
  sample_n(nrow(R_synthetic_tidy)/2) %T>%
  saveRDS("bpmf_shiny/dummy_data/synthetic_simulations_sparse.rds")
sparse_observations %>% 
  mutate(value = sapply(value, transform_score_to_rating)) %>%
  vis_matrix(row_col = "row", column_col = "col",
             value_col = "value", label = TRUE) +
  labs(y = 'Users', x = 'Movies')
```


### BPMF

```{r}
# Implement Gibbs sampler
n_replications <- 1000

# Define initial parameters
k_estimate <- 3
alpha <- 10
mu_0 <- rep(0, k_estimate)
beta_0 <- 1
nu_0 <- 1
W_0 <- diag(k_estimate)

# Run
simulation_results <- BPMF_Gibbs_Sampler(
  sparse_observations, 
  k_estimate, 
  n_replications, 
  n_users, 
  n_movies, 
  mu_0, beta_0, nu_0, W_0, alpha
)

# Take simulated ratings for each user 
simulations.dt <- map2_df(
  .x = expand.grid(1:n_users, 1:n_movies)$Var1,
  .y = expand.grid(1:n_users, 1:n_movies)$Var2,
  .f = function(x,y) {
    extract_simulated_ratings(simulation_results, x,y) %>% 
      mutate(user_index = x) %>% 
      mutate(movie_index = y)
  }
)

# Visualize the mean
simulations.dt %>% group_by(user_index, movie_index) %>% 
  summarise(mean_rating = mean(simulated_rating)) %>% 
  ungroup() %>% 
  mutate(mean_rating = sapply(mean_rating, transform_score_to_rating)) %>% 
  vis_matrix(row_col = "user_index",
             column_col = "movie_index",
             value_col = "mean_rating")

```

```{r}
compute_matrix_difference(
  subtrahend = simulations.dt %>% 
    group_by(movie_index, user_index) %>% 
    summarise(mean = mean(simulated_rating)) %>% 
    ungroup() %>% 
    mutate(mean = sapply(mean, transform_score_to_rating)), 
  minuend = sparse_observations %>% 
    mutate(value = sapply(value, transform_score_to_rating)),
  
  subtrahend_row_col = 'user_index',
  subtrahend_column_col = 'movie_index',
  subtrahend_value_col = 'mean',
  minuend_row_col = 'row',
  minuend_column_col = 'col',
  minuend_value_col = 'value'
) %>% 
  vis_matrix(row_col = 'row',
             column_col = 'col',
             value_col = 'difference') +
  labs(y = 'User', x = 'Movie')
```


## Netflix Data

In this section, we apply the BPMF Gibbs Sampler to our sampled ratings data set on 15 users and 25 movies.

```{r}
set.seed(1)
# Sample ratings
sampled_ratings <- sample_ratings(ratings, 30, 30)
sampled_ratings %>% summarise(n_users=n_distinct(userId),
                              n_movies=n_distinct(movieId),
                              n_ratings=n())
```


```{r}
# Apply mapping from userId -> userKey and movieId -> movieKey
# This allows it to work with the Gibbs Sampler which assumes 1:n_users as
# indices for users and 1:n_movies as indices for movies
sampled_ratings_user_key <- sampled_ratings %>%
  select(userId) %>%
  distinct %>%
  mutate(user_key=row_number())

sampled_ratings_movie_key <- sampled_ratings %>%
  select(movieId) %>%
  distinct %>%
  mutate(movie_key=row_number())

sampled_ratings_mapped <- sampled_ratings %>%
  left_join(sampled_ratings_user_key, by="userId") %>% 
  left_join(sampled_ratings_movie_key, by="movieId") %>% 
  select(user_key, movie_key, rating)

sampled_ratings_mapped %>% head
```

```{r}
sampled_ratings_mapped %>%
  vis_matrix(data = .,
             row_col = "user_key",
             column_col = "movie_key",
             value_col = "rating")
```

```{r}
# Implement Gibbs sampler
n_replications <- 1000

# Define initial parameters
k_estimate <- 10
alpha <- 2
mu_0 <- rep(0, k_estimate)
beta_0 <- 1
nu_0 <- 1
W_0 <- diag(k_estimate)

sampled_ratings_matrix <- sampled_ratings_mapped %>%
  mutate(rating=sapply(rating, transform_rating_to_score)) %>% 
  select(user_key, movie_key, rating) %>%
  as.matrix

n_users <- sampled_ratings_mapped %>%
  summarise(n_users=n_distinct(user_key)) %>%
  as.numeric()

n_movies <- sampled_ratings_mapped %>%
  summarise(n_movies=n_distinct(movie_key)) %>%
  as.numeric()
```

```{r}
# Run
netflix_bpmf_gibbs_results <- BPMF_Gibbs_Sampler(
  sampled_ratings_matrix, 
  k_estimate, 
  n_replications, 
  n_users, 
  n_movies, 
  mu_0, beta_0, nu_0, W_0, alpha
)
```


```{r}
# Visualize posterior untransformed
viz_posterior_rating(16, 8, sampled_ratings_mapped, netflix_bpmf_gibbs_results, transformed = FALSE)
```

```{r}
# Visualize posterior transformed
viz_posterior_rating(16, 8, sampled_ratings_mapped, netflix_bpmf_gibbs_results, transformed = TRUE)
```

```{r}
sampled_ratings_mapped %>% 
  filter(user_key == 16) %>% 
  filter(movie_key == 8)
```

## Model Comparisons and Evaluations

In this section, we compare the predictive performance of BPMF against Probabilistic Matrix Factorization and SVD.

```{r}
# Generate sampled ratings
sampled_ratings <- sample_ratings_top_users(ratings,
                                            n_users=15,
                                            n_movies=15,
                                            max_num_users_per_movie = 10,
                                            min_num_users_per_movie = 6) %>%
  mutate(value=sapply(value, transform_score_to_rating)) %>%
  rename(userId=row,
         movieId=col,
         rating=value)
```
```{r}
# Evaluate models with k from 2 to 15
ks <- 2:15
model_comparisons <- ks %>%
  Map(function(x) {
    model_evaluation_pipeline(sampled_ratings, x) %>% 
      mutate(k=x) %>%
      mutate(rmse_train=rmse_train) %>%
      mutate(rmse_test=rmse_test)}, .) %>%
  Reduce(function(x, y) {rbind(x, y)}, .)

# Plot test RMSE vs. k by model
model_comparisons %>% 
  ggplot(aes(x=k, y=rmse_test, color=model)) + geom_line() + theme_minimal()

```
```{r}
model_comparisons %>% ggplot(aes(x=k, y=rmse_test, color=model)) + geom_line() + theme_minimal()
```

